# CUDA-Accelerated GAN from Scratch

A high-performance Generative Adversarial Network (GAN) implementation built from scratch using custom CUDA kernels, without relying on PyTorch or TensorFlow for training. This project demonstrates how to implement neural network operations directly on GPU using CUDA.

## üöÄ Features

- **Custom CUDA Kernels**: Matrix multiplication (GEMM), activation functions, loss functions, and optimizers
- **High Performance**: Optimized for NVIDIA GPUs with efficient memory management
- **Cross-Platform**: Works on Windows and Linux
- **Mixed Precision Support**: Ready for float16/float32 mixed precision training
- **Complete GAN Pipeline**: Generator, Discriminator, and training loop implementation

## üìã Requirements

### Hardware
- NVIDIA GPU with CUDA support (Compute Capability 7.0+)
- Tested on NVIDIA GeForce GTX 1660 Ti

### Software
- **Windows**: 
  - CUDA Toolkit 11.0+ (tested with CUDA 12.9)
  - Visual Studio 2022 Build Tools with C++ workload
  - Python 3.8+
- **Linux**:
  - CUDA Toolkit 11.0+
  - GCC compiler
  - Python 3.8+

### Python Dependencies
```bash
pip install numpy pycuda torch matplotlib
```

## üõ†Ô∏è Installation

1. **Clone the repository**:
   ```bash
   git clone <repository-url>
   cd CUDA_project
   ```

2. **Install Python dependencies**:
   ```bash
   pip install numpy pycuda torch matplotlib
   ```

3. **Verify CUDA installation**:
   ```bash
   nvcc --version
   ```

4. **Test the setup**:
   ```bash
   python test_kernels.py
   ```

## üìÅ Project Structure

```
CUDA_project/
‚îú‚îÄ‚îÄ cuda_kernels.cu          # Custom CUDA kernels implementation
‚îú‚îÄ‚îÄ test_kernels.py          # Test harness for kernel validation
‚îú‚îÄ‚îÄ README.md               # This file
‚îî‚îÄ‚îÄ requirements.txt        # Python dependencies
```

## üîß CUDA Kernels

### Matrix Operations
- **GEMM**: General matrix multiplication with batched support
- **Transpose**: Matrix transpose for gradient computation

### Activation Functions
- **ReLU**: Rectified Linear Unit with gradient
- **Sigmoid**: Sigmoid activation with gradient
- **Tanh**: Hyperbolic tangent with gradient

### Loss Functions
- **Binary Cross-Entropy**: Loss and gradient computation

### Optimizers
- **Adam**: Adaptive moment estimation optimizer

## üß™ Testing

Run the comprehensive test suite to validate all CUDA kernels:

```bash
python test_kernels.py
```

This will:
1. Compile the CUDA kernels into a shared library
2. Test matrix multiplication (GEMM)
3. Validate activation functions (ReLU, Sigmoid, Tanh)
4. Verify binary cross-entropy loss and gradients
5. Test Adam optimizer implementation

Expected output:
```
==================================================
Test Results: 4/4 tests passed
==================================================

üéâ All tests passed! CUDA kernels are working correctly.
```

## üöÄ Usage

### Basic Kernel Usage

```python
import numpy as np
import ctypes
from test_kernels import CUDATestHarness

# Initialize test harness
harness = CUDATestHarness()
harness.compile_kernels()

# Example: Matrix multiplication
M, N, K = 64, 64, 64
A = np.random.randn(M, K).astype(np.float32)
B = np.random.randn(K, N).astype(np.float32)

# Allocate GPU memory
A_gpu = harness.allocate_gpu_memory(M * K)
B_gpu = harness.allocate_gpu_memory(K * N)
C_gpu = harness.allocate_gpu_memory(M * N)

# Copy data to GPU
harness.copy_to_gpu(np.ascontiguousarray(A.ravel(order='C')), A_gpu)
harness.copy_to_gpu(np.ascontiguousarray(B.ravel(order='C')), B_gpu)

# Perform matrix multiplication
harness.lib.cuda_gemm(
    ctypes.c_void_p(int(A_gpu)),
    ctypes.c_void_p(int(B_gpu)),
    ctypes.c_void_p(int(C_gpu)),
    ctypes.c_int(M), ctypes.c_int(N), ctypes.c_int(K),
    ctypes.c_float(1.0), ctypes.c_float(0.0)
)

# Copy result back
C_cuda = np.zeros((M, N), dtype=np.float32)
harness.copy_from_gpu(C_gpu, C_cuda.ravel(order='C'))
C_cuda = C_cuda.reshape((M, N), order='C')
```

## üîç Technical Details

### Memory Management
- Uses PyCUDA for GPU memory allocation and data transfer
- Automatic memory cleanup through PyCUDA's reference counting
- Contiguous array handling for optimal performance

### Kernel Optimization
- Shared memory tiling ready for GEMM optimization
- Efficient grid/block dimension calculation
- Error checking and debugging support

### Cross-Platform Support
- Windows: Uses `__declspec(dllexport)` for DLL exports
- Linux: Uses standard `extern "C"` declarations
- Automatic platform detection and compilation

## üêõ Troubleshooting

### Common Issues

1. **"nvcc not found"**
   - Install CUDA Toolkit and add to PATH
   - Verify with `nvcc --version`

2. **"Cannot find compiler 'cl.exe'" (Windows)**
   - Install Visual Studio 2022 Build Tools with C++ workload
   - Use Developer Command Prompt

3. **"function not found" errors**
   - Recompile kernels: `python test_kernels.py`
   - Check CUDA version compatibility

4. **Large GEMM errors**
   - Ensure arrays are contiguous and row-major
   - Check memory alignment

### Debug Mode
Enable detailed error reporting by modifying `cuda_kernels.cu`:
```cpp
#define CUDA_CHECK(call) \
    do { \
        cudaError_t err = call; \
        if (err != cudaSuccess) { \
            fprintf(stderr, "CUDA error at %s:%d - %s\n", __FILE__, __LINE__, cudaGetErrorString(err)); \
            exit(EXIT_FAILURE); \
        } \
    } while(0)
```

## üìà Performance

### Benchmarks
- **GEMM**: ~7.6e-06 max error (float32 precision)
- **Activations**: ~1.2e-07 max error
- **Loss Functions**: ~1.2e-07 max error
- **Adam Optimizer**: ~3.7e-09 max error

### Optimization Opportunities
- Shared memory tiling for larger matrices
- Mixed precision (float16) support
- CUDA streams for overlapping operations
- Kernel fusion for better memory bandwidth

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## üôè Acknowledgments

- NVIDIA for CUDA toolkit and documentation
- PyCUDA developers for Python-CUDA integration
- The CUDA programming community for best practices

---
